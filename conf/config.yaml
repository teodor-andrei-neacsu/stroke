ds_params:
  data_path: "./Keystrokes/"
  user_cnt: 2000 # number of users
  max_seq_len: 50 # max sequence length of typing sequence
  key_cnt: 256 # number of possible keys
  test_size: 3 # number of test samples /15
  feat_cnt: 4 # number of features
  num_workers: 4 # number of workers for dataloader
  replace_prob: 0.5 # 
  user_prob: 0 # ratio of samples with replaced tokens 
  dataset_multiplier: 5 # number of times to repeat the dataset

model_params:
  key_emb_size: 256 # key embedding size
  dim_ff: 512 # feed forward dimension
  num_heads: 12 # number of heads in multi-head attention
  num_layers: 6 # number of layers in transformer
  trf_dropout: 0.2 # transformer dropout
  causal_att: True # whether to use causal attention
  use_user_emb: False # whether to use user embedding (appended)

training_params:
  batch_size: 128 # training batch size
  val_batch_size: 8 # validation batch size
  dataset_multiplier: 4 # number of times to repeat the dataset
  max_epochs: 1000 # number of epochs
  lr: 0.0001 # learning rate
  check_val_every_n_epoch: 200
  num_gpus: 1
  train: True
  wandb_log: True
  wandb_project: move_gpu_util
  wandb_name: key-hyper

